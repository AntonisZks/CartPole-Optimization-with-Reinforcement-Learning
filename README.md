<div align="center">
    <h1><b>CartPole Environment with Reinforcement Learning</b></h1>
</div>

<div align="center">

![License](https://img.shields.io/github/license/AntonisZks/Reinforcement-Learning-Assignment.svg)
![Repository Size](https://img.shields.io/github/repo-size/AntonisZks/Reinforcement-Learning-Assignment.svg)
![Release](https://img.shields.io/github/v/release/AntonisZks/Reinforcement-Learning-Assignment.svg)
![Contributors](https://img.shields.io/github/contributors/AntonisZks/Reinforcement-Learning-Assignment.svg)
	
</div>

## ğŸ“‹ Table of Contents
- [Team Members](#team-members)
- [Project Overview](#project-overview)
- [Environment Analysis](#environment-analysis)
- [Implementation](#implementation)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Usage](#usage)
- [Results](#results)
- [Models and Architectures](#models-and-architectures)
- [Contributing](#contributing)

## ğŸ‘¥ Team Members

<div align="center">

| First Name | Last Name | Student ID | Email |
|:---:|:---:|:---:|:---:|
| Antonis | Zikas | 1115202100038 | sdi2100038@di.uoa.gr |
| Panagiotis | Papapostolou | 1115202100142 | sdi2100142@di.uoa.gr |

</div>

## ğŸ¯ Project Overview

This project implements and experiments with various **Reinforcement Learning algorithms** to train agents on the **CartPole-v1** environment from OpenAI Gymnasium. The main focus is on **Deep Q-Network (DQN)** implementations with different architectural variations and comparative analysis with other RL algorithms.

### Key Features
- ğŸ§  **Multiple DQN Implementations**: Standard DQN, Dueling Architecture, and Transformer-based Q-Networks
- ğŸ“Š **Comprehensive Analysis**: Performance comparison with random actions and sensitivity studies
- ğŸ”§ **Modular Design**: Clean, well-documented code structure
- ğŸ“ˆ **Visualization**: Detailed plotting and analysis of training results
- ğŸ® **Environment Testing**: Baseline performance analysis with random actions
- ğŸ† **State-of-the-art Algorithms**: Integration with Stable-Baselines3 (PPO, A2C)

## ğŸ® Environment Analysis

### CartPole-v1 Environment
The CartPole-v1 environment is a classic control problem where the goal is to balance a pole on a cart by moving the cart left or right.

#### Action Space
- **Discrete**: 2 possible actions
  - `0`: Move cart to the left
  - `1`: Move cart to the right

#### Observation Space
- **Continuous**: 4-dimensional state vector
  - `[0]`: Cart Position (range: -4.8 to 4.8)
  - `[1]`: Cart Velocity (range: -âˆ to +âˆ)
  - `[2]`: Pole Angle (range: ~-0.418 to 0.418 radians)
  - `[3]`: Pole Angular Velocity (range: -âˆ to +âˆ)

#### Reward System
- **+1** for each timestep the pole remains upright
- **Reward threshold**: 500 (considered solved)
- Episode terminates when pole angle > Â±12Â° or cart position > Â±2.4

## ğŸš€ Implementation

### Core Algorithms Implemented

1. **Deep Q-Network (DQN)**
   - Experience replay buffer
   - Target network for stable learning
   - Îµ-greedy exploration strategy

2. **Dueling Architecture DQN**
   - Separate value and advantage streams
   - Improved learning efficiency

3. **Transformer-based Q-Network**
   - Sequential state processing
   - Attention mechanism for temporal dependencies

4. **Stable-Baselines3 Integration**
   - Proximal Policy Optimization (PPO)
   - Advantage Actor-Critic (A2C)

### Key Components

- **Neural Networks**: Fully connected layers with ReLU activations
- **Replay Buffer**: Experience replay for stable training  
- **Target Networks**: Periodic updates for learning stability
- **Exploration Strategy**: Îµ-greedy with exponential decay

## ğŸ“ Project Structure

```
Reinforcement-Learning-Assignment/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ cart_pole.ipynb          # Main Jupyter notebook with experiments
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents.py                # DQN Agent implementation
â”‚   â”œâ”€â”€ networks.py              # Neural network architectures
â”‚   â”œâ”€â”€ trainers.py              # Training logic and utilities
â”‚   â”œâ”€â”€ replay_buffers.py        # Experience replay buffer
â”‚   â”œâ”€â”€ testing.py               # Model testing and evaluation
â”‚   â”œâ”€â”€ plotting.py              # Visualization utilities
â”‚   â”œâ”€â”€ utils.py                 # Helper functions and hyperparameters
â”‚   â”œâ”€â”€ dqn.py                   # Main DQN training script
â”‚   â”œâ”€â”€ env_showcase.py          # Environment demonstration
â”‚   â”œâ”€â”€ stable_baselines_a2c.py  # A2C training with Stable-Baselines3
â”‚   â””â”€â”€ stable_baselines_ppo.py  # PPO training with Stable-Baselines3
â”‚
â”œâ”€â”€ models/                      # Saved trained models
â”‚   â”œâ”€â”€ dqn_model.pth
â”‚   â”œâ”€â”€ dueling_arc_dqn_model.pth
â”‚   â”œâ”€â”€ transformer_model.pth
â”‚   â”œâ”€â”€ ppo_*.pth
â”‚   â””â”€â”€ a2c_*.pth
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ figs/                    # Generated plots and visualizations
â”‚   â””â”€â”€ PDFs/                    # Final report documents
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ tensorboard/             # TensorBoard logging for training metrics
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ imgs/                    # Images and diagrams
â”‚
â”œâ”€â”€ docs/                        # Assignment documentation
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # This file
```

## ğŸ”§ Installation

### Prerequisites
- Python 3.8+
- CUDA-compatible GPU (optional, for faster training)

### Setup Instructions

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd Reinforcement-Learning-Assignment
   ```

2. **Create virtual environment** (recommended)
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

### Key Dependencies
- `torch`: Deep learning framework
- `gymnasium`: OpenAI Gym environments
- `stable-baselines3`: State-of-the-art RL algorithms
- `matplotlib`: Plotting and visualization
- `numpy`: Numerical computations
- `jupyter`: Interactive notebooks

## ğŸ® Usage

### Quick Start

1. **Run Environment Showcase**
   ```bash
   python src/env_showcase.py
   ```

2. **Train DQN Agent**
   ```bash
   python src/dqn.py
   ```

3. **Train with Stable-Baselines3**
   ```bash
   python src/stable_baselines_ppo.py  # For PPO
   python src/stable_baselines_a2c.py  # For A2C
   ```

4. **Interactive Analysis**
   ```bash
   jupyter notebook notebooks/cart_pole.ipynb
   ```

### Hyperparameter Configuration

Key hyperparameters are defined in `src/utils.py`:

```python
GAMMA = 0.99          # Discount factor
LR = 1e-3             # Learning rate
BATCH_SIZE = 64       # Minibatch size
MEMORY_SIZE = 10000   # Replay buffer size
EPSILON_START = 1.0   # Starting exploration probability
EPSILON_END = 0.01    # Minimum exploration probability
EPSILON_DECAY = 0.995 # Epsilon decay rate
TARGET_UPDATE = 10    # Target network update frequency
```

## ğŸ“Š Results

### Performance Comparison

| Algorithm | Average Score | Success Rate | Training Episodes |
|-----------|---------------|--------------|------------------|
| Random Actions | ~22 | ~10% | N/A |
| DQN | ~475+ | ~95%+ | 500 |
| Dueling DQN | ~480+ | ~96%+ | 500 |
| Transformer DQN | ~450+ | ~90%+ | 500 |
| PPO (Stable-Baselines3) | ~500 | ~99% | Variable |
| A2C (Stable-Baselines3) | ~495+ | ~98% | Variable |

### Key Findings
- âœ… All implemented algorithms significantly outperform random actions
- âœ… Dueling architecture shows slight improvement over standard DQN
- âœ… Stable-Baselines3 implementations achieve near-optimal performance
- âœ… Transformer-based approach shows promise but requires tuning

## ğŸ§  Models and Architectures

### Standard DQN Architecture
```
Input Layer (4 nodes) â†’ Hidden Layer (128) â†’ Hidden Layer (128) â†’ Hidden Layer (128) â†’ Output Layer (2 nodes)
```

### Dueling Architecture
- **Shared layers**: 4 â†’ 128 â†’ 128 â†’ 128 â†’ 128
- **Value stream**: 128 â†’ 64 â†’ 1
- **Advantage stream**: 128 â†’ 64 â†’ 2
- **Combination**: Q(s,a) = V(s) + (A(s,a) - mean(A(s,Â·)))

### Transformer Architecture
- **Sequence length**: 10 timesteps
- **Embedding dimension**: 64
- **Attention heads**: 4
- **Encoder layers**: 2

## ğŸ“ˆ Monitoring and Visualization

The project includes comprehensive visualization tools:

- **Training Progress**: Score and epsilon decay over episodes
- **Performance Comparison**: Trained agents vs. random actions
- **Sensitivity Analysis**: Hyperparameter impact studies
- **TensorBoard Integration**: Real-time training metrics

## ğŸ¤ Contributing

This is an academic project for coursework. The implementation follows best practices for:

- **Code Organization**: Modular, well-documented structure
- **Reproducibility**: Seed setting for consistent results
- **Experimentation**: Comprehensive sensitivity studies
- **Visualization**: Clear, informative plots and metrics

---

<div align="center">
    <p><em>This project is part of the coursework for Reinforcement Learning & Stochastic Games</em></p>
    <p><strong>National and Kapodistrian University of Athens</strong></p>
    <p>Department of Informatics and Telecommunications</p>
</div>